{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "module_path = \"/home/jacky/Desktop/qhack_2023\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from algorithms.q_learning import QLearning\n",
    "from algorithms.qrl_classic import QRLClassic\n",
    "from algorithms.custom_eval_callback import CustomEvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9074c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch as th\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "\n",
    "from algorithms.custom_eval_callback import CustomEvalCallback\n",
    "\n",
    "\n",
    "class TDLambda(BaseAlgorithm):\n",
    "    \"\"\"\n",
    "    TD-Lambda\n",
    "    NOTE: this implementation has some unused parameters because we inherit from the SB3 BaseAlgorithm class.\n",
    "    \"\"\"\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: str,\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 7e-4,\n",
    "        gamma: float = 0.95,  # Discounting rate\n",
    "        lamb: float = 0.95,  # Lambda Discounting rate\n",
    "        max_steps: int = 99,  # Max steps per episode\n",
    "        max_epsilon: float = 1.0,  # Exploration probability at start\n",
    "        min_epsilon: float = 0.05,  # Minimum exploration probability\n",
    "        decay_rate: float = 0.0005,  # Exponential decay rate for exploration prob\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # Agent parameters\n",
    "        self.gamma = gamma\n",
    "        self.lamb = lamb\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "        state_space_n = self.observation_space.n\n",
    "        action_space_n = self.action_space.n\n",
    "        self.q_table = np.zeros((state_space_n, action_space_n))\n",
    "        self.eligibility_trace = np.zeros((state_space_n, action_space_n))\n",
    "\n",
    "    def learn(\n",
    "        self,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 100,\n",
    "        tb_log_name: str = \"TDLambda\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ):\n",
    "        total_timesteps, callback = self._setup_learn(\n",
    "            total_timesteps,\n",
    "            callback,\n",
    "            reset_num_timesteps,\n",
    "            tb_log_name,\n",
    "            progress_bar,\n",
    "        )\n",
    "\n",
    "        step = 0\n",
    "        episode_n = 0\n",
    "        while step < total_timesteps:\n",
    "            episode_n += 1\n",
    "            episode_step = 0\n",
    "            episode_done = False\n",
    "            # Reduce epsilon (because we need less and less exploration)\n",
    "            epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * episode_n)\n",
    "            state = self.env.reset()\n",
    "\n",
    "            # Run an episode\n",
    "            while not episode_done or episode_step < self.max_steps:\n",
    "                step += 1\n",
    "                episode_step += 1\n",
    "\n",
    "                action = self._epsilon_greedy_policy(state, epsilon)\n",
    "\n",
    "                # Take action At and observe Rt+1 and St+1\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, episode_done, info = self.env.step(action)\n",
    "                self._update_eligibility_table(state, action)\n",
    "                self._update_q_table(state, action, new_state, reward)\n",
    "\n",
    "                # Our next state is the new state\n",
    "                state = new_state\n",
    "\n",
    "                if isinstance(callback, CustomEvalCallback):\n",
    "                    if step % callback.eval_freq == 0:\n",
    "                        callback.on_step()\n",
    "\n",
    "    def _greedy_policy(self, state):\n",
    "        # Exploitation: take the action with the highest state, action value\n",
    "        action = np.array([np.argmax(self.q_table[s][:]) for s in state])\n",
    "        return action\n",
    "\n",
    "    def _epsilon_greedy_policy(self, state, epsilon):\n",
    "        # Randomly generate a number between 0 and 1\n",
    "        random_int = np.random.uniform(0, 1)\n",
    "        # if random_int > greater than epsilon --> exploitation\n",
    "        if random_int > epsilon:\n",
    "            action = self._greedy_policy(state)\n",
    "        # else --> exploration\n",
    "        else:\n",
    "            action = [self.action_space.sample() for _ in state]\n",
    "        return action\n",
    "\n",
    "    def _update_q_table(self, state, action, new_state, reward):\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        state_idx = state[0]\n",
    "        new_state_idx = new_state[0]\n",
    "        action_idx = action[0]\n",
    "        reward_idx = reward[0]\n",
    "        td_error = reward_idx + self.gamma * np.max(self.q_table[new_state_idx]) - self.q_table[state_idx][action_idx]\n",
    "        self.q_table = self.q_table + self.learning_rate * td_error * self.eligibility_trace\n",
    "        \n",
    "    def _update_eligibility_table(self, state, action):\n",
    "        state_idx = state[0]\n",
    "        action_idx = action[0]\n",
    "        self.eligibility_trace *= self.lamb * self.gamma\n",
    "        self.eligibility_trace[state_idx][action_idx] += 1.0\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
    "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
    "        episode_start: Optional[np.ndarray] = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
    "        if deterministic:\n",
    "            action = self._greedy_policy(observation)\n",
    "            return action, state\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    model_name,\n",
    "    model_parameters: dict,\n",
    "    random_seed: int = 542,\n",
    "    env_map_size: int = 10,\n",
    "    env_non_deterministic: bool = False,\n",
    "    eval_freq: int = 100,\n",
    "    n_eval_episodes: int = 20,\n",
    "    total_timesteps: int = 1_000_000\n",
    "):\n",
    "    np.random.seed(seed=random_seed)\n",
    "    \n",
    "    random_map = generate_random_map(size=env_map_size, p=0.8)\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=env_non_deterministic)\n",
    "    eval_env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=env_non_deterministic)\n",
    "        \n",
    "    callback = CustomEvalCallback(\n",
    "        eval_env=eval_env,\n",
    "        eval_freq=eval_freq,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        verbose=0\n",
    "    )\n",
    "        \n",
    "    model = model(\n",
    "        policy = None,\n",
    "        env = env,\n",
    "        **model_parameters,\n",
    "    )\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback = callback    \n",
    "    )\n",
    "    \n",
    "    learning_curve = callback.learning_curve\n",
    "    eval_freq = callback.eval_freq\n",
    "    \n",
    "    return eval_freq, learning_curve\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 542\n",
    "np.random.seed(seed=random_seed)\n",
    "random_seeds = [np.random.randint(1000) for _ in range(10)]\n",
    "\n",
    "env_non_deterministic = False\n",
    "env_map_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "total_timesteps = 10000  # Total training steps\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "eval_freq = 100\n",
    "\n",
    "# Environment parameters\n",
    "max_steps = 200              # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "lamb = 0.0005                  # Lambda rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.00005              # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "\n",
    "model_parameters = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "    \"lamb\": lamb,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "    \"verbose\": 0,\n",
    "    \"seed\": None,\n",
    "    \"device\": \"auto\",\n",
    "    \"_init_setup_model\": False,    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "learning_curves_tdl = list()\n",
    "for rs in random_seeds:\n",
    "    _, lc = run_experiment(\n",
    "        model=TDLambda,\n",
    "        model_name=\"TDLambda_deterministic\",\n",
    "        model_parameters=model_parameters,\n",
    "        env_non_deterministic=env_non_deterministic,\n",
    "        env_map_size=env_map_size,\n",
    "        eval_freq=eval_freq,\n",
    "        total_timesteps = total_timesteps,\n",
    "        random_seed=rs,\n",
    "    )\n",
    "    learning_curves_tdl.append(lc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "total_timesteps = 10000  # Total training steps\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "eval_freq = 100\n",
    "\n",
    "# Environment parameters\n",
    "max_steps = 200              # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01              # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "\n",
    "model_parameters = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "    \"verbose\": 0,\n",
    "    \"seed\": None,\n",
    "    \"device\": \"auto\",\n",
    "    \"_init_setup_model\": False,    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "learning_curves_ql = list()\n",
    "for rs in random_seeds:\n",
    "    _, lc = run_experiment(\n",
    "        model=QLearning,\n",
    "        model_name=\"ql_deterministic\",\n",
    "        model_parameters=model_parameters,\n",
    "        env_non_deterministic=env_non_deterministic,\n",
    "        env_map_size=env_map_size,\n",
    "        eval_freq=eval_freq,\n",
    "        total_timesteps = total_timesteps,\n",
    "        random_seed=rs,\n",
    "    )\n",
    "    learning_curves_ql.append(lc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b13153",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, shared_yaxes=True, subplot_titles=(\"TD-Lambda\", \"QL\"))\n",
    "\n",
    "\n",
    "cutoff = 100\n",
    "for i, lc in enumerate(learning_curves_tdl):\n",
    "    lc = lc[:cutoff]\n",
    "    mean_reward = [c[0] for c in lc]\n",
    "    x = [e*eval_freq for e in range(len(mean_reward))]\n",
    "        \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=mean_reward,\n",
    "            mode=\"lines\",\n",
    "            name=f\"ql_mean_reward_{i}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )   \n",
    "    \n",
    "for i, lc in enumerate(learning_curves_ql):\n",
    "    lc = lc[:cutoff]    \n",
    "    mean_reward = [c[0] for c in lc]\n",
    "    x = [e*eval_freq for e in range(len(mean_reward))]\n",
    "        \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=mean_reward,\n",
    "            mode=\"lines\",\n",
    "            name=f\"qrl_mean_reward_{i}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )   \n",
    "    \n",
    "fig.update_layout(width=1200, height=600, showlegend=True, title_text=f\"TD-Lambda vs QL learning curves (map size {env_map_size} random {env_non_deterministic})\")\n",
    "fig.show()    \n",
    "plot_name = f\"../images/ql_vs_qrl_size_{env_map_size}_random_{env_non_deterministic}\"\n",
    "fig.write_html(f\"{plot_name}.html\")\n",
    "fig.write_image(f\"{plot_name}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
